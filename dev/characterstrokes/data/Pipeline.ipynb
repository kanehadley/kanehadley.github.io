{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "from PIL import Image\n",
    "from PIL import ImageOps\n",
    "\n",
    "#import numpy as np\n",
    "import autograd.numpy as np\n",
    "\n",
    "def calculate_edges(img, size):\n",
    "    row_coord, col_coord = np.nonzero(-(np.matrix(img.getdata()).reshape(size)) + 255)\n",
    "    # Left, Top, Right, Bottom\n",
    "    return np.min(col_coord) - 1, np.min(row_coord) - 1, np.max(col_coord) + 1, np.max(row_coord) + 1\n",
    "\n",
    "def calculate_normalizing_dimensions(image, theta):\n",
    "    rotated_image = ImageOps.crop(ImageOps.expand(image, 750, 255).rotate(theta), 500)\n",
    "    sides = calculate_edges(rotated_image, (1000, 1000))\n",
    "    return sides\n",
    "\n",
    "def generate_sample(image, theta, normalizing_dimensions):\n",
    "    \"\"\"Rotates an image, downsample it to a 28x28 sample, and then reatures the 28*28 feature vector.\n",
    "    \n",
    "    :param - img\n",
    "        The image to be transformed and downsampled.\n",
    "        \n",
    "    :param - theta\n",
    "        The angle to transform the image before downsampling.\n",
    "    \"\"\"\n",
    "    rotated_image = ImageOps.crop(ImageOps.expand(image, 750, 255).rotate(theta), 500)\n",
    "    \n",
    "    #sides = calculate_edges(rotated_image, (1000, 1000))\n",
    "    \n",
    "    # Image is 1000 x 1000\n",
    "    # normalizing_dimensions is width x height\n",
    "    #width_difference = 1000 - normalizing_dimensions[0]\n",
    "    #height_difference = 1000 - normalizing_dimensions[1]\n",
    "    #expansion = max(width_difference, height_difference) / 2\n",
    "    #compression = min(width_difference, height_difference)\n",
    "    #expansion_image = ImageOps.expand(rotated_image, expansion, 255)\n",
    "    \n",
    "    #if width_difference < height_difference:\n",
    "    #    side_left = (height_difference - width_difference) / 2\n",
    "    #    side_top = 0\n",
    "    #    side_right = expansion_image.size[0] - ((height_difference - width_difference) / 2)\n",
    "    #    side_bottom = expansion_image.size[1]\n",
    "    #else:\n",
    "    #    side_left = 0\n",
    "    #    side_top = (width_difference - height_difference) / 2\n",
    "    #    side_right = expansion_image.size[0]\n",
    "    #    side_bottom = expansion_image.size[1] - ((width_difference - height_difference) / 2)\n",
    "    \n",
    "    #sides = (side_left, side_top, side_right, side_bottom)\n",
    "    \n",
    "    #cropped_image = rotated_image.crop(sides)\n",
    "\n",
    "    cropped_image = rotated_image.crop(normalizing_dimensions)\n",
    "    \n",
    "    # Makes it a square\n",
    "    cropped_max_size = max(cropped_image.size)\n",
    "    \n",
    "    cropped_difference_half = int(((1.0 * cropped_max_size) - min(cropped_image.size)) / 2)\n",
    "    \n",
    "    expanded_cropped_image = ImageOps.expand(cropped_image, cropped_difference_half, 255)\n",
    "    \n",
    "    expanded_cropped_image_size = expanded_cropped_image.size\n",
    "    \n",
    "    # Width is greater\n",
    "    if cropped_image.size[0] > cropped_image.size[1]:\n",
    "        cropped_expanded_cropped_image = expanded_cropped_image.crop((cropped_difference_half,\n",
    "                                                                     0,\n",
    "                                                                     expanded_cropped_image_size[0] - cropped_difference_half,\n",
    "                                                                     expanded_cropped_image_size[1]))\n",
    "    # Height is greater\n",
    "    else:\n",
    "        cropped_expanded_cropped_image = expanded_cropped_image.crop((0,\n",
    "                                                                     cropped_difference_half,\n",
    "                                                                     expanded_cropped_image_size[0],\n",
    "                                                                     expanded_cropped_image_size[1] - cropped_difference_half))\n",
    "    \n",
    "    \n",
    "    #return cropped_expanded_cropped_image\n",
    "    return (-np.array(cropped_expanded_cropped_image.resize((28,28)).getdata()) + 255) * 1.0 / 255\n",
    "    #img_data = np.matrix(list(rotated_img.getdata()))\n",
    "    \n",
    "    \n",
    "    #d = c.reshape((2000, 2000))\n",
    "    #e = (-d) % 255\n",
    "    #e = ImageOps.crop(ImageOps.expand(a, 250, 255).rotate(45), 146)\n",
    "    #math.floor((1000 - 500*math.sqrt(2)) / 2)\n",
    "\n",
    "    #scaled_img = rotated_img.resize((128,128))\n",
    "    \n",
    "    #return scaled_img\n",
    "\n",
    "example = Image.open('characters/traditional_dragon_15.png')\n",
    "example2 = Image.open('characters/traditional_dragon_03.png')\n",
    "\n",
    "z = generate_sample(example2, 90, calculate_normalizing_dimensions(example, 90))\n",
    "#Image.fromarray((-(z.reshape(28,28) * 255) + 255).astype('uint8'), 'L')\n",
    "#z.resize((28,28))\n",
    "#z.shape\n",
    "#z\n",
    "#calculate_prescale_dimensions(example, 90)\n",
    "#y = np.array(z.getdata())\n",
    "#y.size\n",
    "#z.size\n",
    "#Image.fromarray(y.reshape(445,445), 'I')\n",
    "#Image.frombuffer('I', (445, 445), y, 'raw', 'I', 0, 255)\n",
    "#Image.fromarray(np.array(z.getdata()).astype('uint8').reshape((445,445)))\n",
    "\n",
    "def view_feature_array(array, size):\n",
    "    return Image.fromarray((-(array.reshape(size) * 255) + 255).astype('uint8'), 'L')\n",
    "\n",
    "#view_feature_array(z, (28, 28))\n",
    "\n",
    "#view_feature_array(np.array([generate_sample(example, 90, calculate_normalizing_dimensions(example, 90)),\n",
    "#         generate_sample(example2, 90, calculate_normalizing_dimensions(example, 90))])[1],(28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#view_feature_array(z3[1], (28,28))\n",
    "\n",
    "name_class_list = [\n",
    "    ('traditional_dragon_00.png', 0),\n",
    "    ('traditional_dragon_01.png', 1),\n",
    "    ('traditional_dragon_02.png', 2),\n",
    "    ('traditional_dragon_03.png', 3),\n",
    "    ('traditional_dragon_04.png', 4),\n",
    "    ('traditional_dragon_05.png', 5),\n",
    "    ('traditional_dragon_06.png', 6),\n",
    "    ('traditional_dragon_07.png', 7),\n",
    "    ('traditional_dragon_08.png', 8),\n",
    "    ('traditional_dragon_09.png', 9),\n",
    "    ('traditional_dragon_10.png', 10),\n",
    "    ('traditional_dragon_11.png', 11),\n",
    "    ('traditional_dragon_12.png', 12),\n",
    "    ('traditional_dragon_13.png', 13),\n",
    "    ('traditional_dragon_14.png', 14),\n",
    "    ('traditional_dragon_15.png', 15)]\n",
    "#    ('simplified_dragon_00.png', 16),\n",
    "#    ('simplified_dragon_01.png', 17),\n",
    "#    ('simplified_dragon_02.png', 18),\n",
    "#    ('simplified_dragon_03.png', 19),\n",
    "#    ('simplified_dragon_04.png', 20)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image_class_list = map(lambda x: (Image.open('characters/' + x[0]), x[1]), name_class_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating training data\n",
      "Generating images for image 1  out of 16\n",
      "Generated 10 samples out of 10 .\n",
      "Generating images for image 2  out of 16\n",
      "Generated 10 samples out of 10 .\n",
      "Generating images for image 3  out of 16\n",
      "Generated 10 samples out of 10 .\n",
      "Generating images for image 4  out of 16\n",
      "Generated 10 samples out of 10 .\n",
      "Generating images for image 5  out of 16\n",
      "Generated 10 samples out of 10 .\n",
      "Generating images for image 6  out of 16\n",
      "Generated 10 samples out of 10 .\n",
      "Generating images for image 7  out of 16\n",
      "Generated 10 samples out of 10 .\n",
      "Generating images for image 8  out of 16\n",
      "Generated 10 samples out of 10 .\n",
      "Generating images for image 9  out of 16\n",
      "Generated 10 samples out of 10 .\n",
      "Generating images for image 10  out of 16\n",
      "Generated 10 samples out of 10 .\n",
      "Generating images for image 11  out of 16\n",
      "Generated 10 samples out of 10 .\n",
      "Generating images for image 12  out of 16\n",
      "Generated 10 samples out of 10 .\n",
      "Generating images for image 13  out of 16\n",
      "Generated 10 samples out of 10 .\n",
      "Generating images for image 14  out of 16\n",
      "Generated 10 samples out of 10 .\n",
      "Generating images for image 15  out of 16\n",
      "Generated 10 samples out of 10 .\n",
      "Generating images for image 16  out of 16\n",
      "Generated 10 samples out of 10 .\n",
      "Generating testing data\n",
      "Generating images for image 1  out of 16\n",
      "Generating images for image 2  out of 16\n",
      "Generating images for image 3  out of 16\n",
      "Generating images for image 4  out of 16\n",
      "Generating images for image 5  out of 16\n",
      "Generating images for image 6  out of 16\n",
      "Generating images for image 7  out of 16\n",
      "Generating images for image 8  out of 16\n",
      "Generating images for image 9  out of 16\n",
      "Generating images for image 10  out of 16\n",
      "Generating images for image 11  out of 16\n",
      "Generating images for image 12  out of 16\n",
      "Generating images for image 13  out of 16\n",
      "Generating images for image 14  out of 16\n",
      "Generating images for image 15  out of 16\n",
      "Generating images for image 16  out of 16\n"
     ]
    }
   ],
   "source": [
    "def generate_data_sets(image_class_pairs, test_set_size):\n",
    "    train_set_size = 10 * test_set_size\n",
    "    sub_image_pairs = image_class_pairs[:-1]\n",
    "    main_image_pair = image_class_pairs[-1]\n",
    "    \n",
    "    mu = 0\n",
    "    sigma = 0.1\n",
    "    \n",
    "    \n",
    "    # Creating training data\n",
    "    print 'Generating training data'\n",
    "    aggregated_generated_image_list = []\n",
    "    aggregated_generated_class_list = []\n",
    "    \n",
    "    for pair_index, pair in enumerate(image_class_pairs):\n",
    "        print 'Generating images for image', (pair_index + 1), ' out of', len(image_class_pairs)\n",
    "        \n",
    "        sampled_rotations = np.random.normal(mu, sigma, train_set_size) * 360\n",
    "        generated_image_list = []\n",
    "        generated_class_list = []\n",
    "        \n",
    "        for index, theta in enumerate(sampled_rotations):\n",
    "            normalizing_size = calculate_normalizing_dimensions(main_image_pair[0], theta)\n",
    "            sample = generate_sample(pair[0],\n",
    "                                     theta,\n",
    "                                     normalizing_size)\n",
    "            generated_image_list.append(sample)\n",
    "            generated_class_list.append(pair[1])\n",
    "            if 0 == (index + 1) % 10:\n",
    "                print 'Generated', (index + 1), 'samples out of', train_set_size, '.'\n",
    "    \n",
    "        combined_generated_image_array = np.vstack(generated_image_list)\n",
    "        aggregated_generated_image_list.append(combined_generated_image_array)\n",
    "        \n",
    "        combined_generated_class_array = np.array(generated_class_list)\n",
    "        aggregated_generated_class_list.append(combined_generated_class_array)\n",
    "        \n",
    "    aggregated_generated_image_array = np.vstack(aggregated_generated_image_list)\n",
    "    aggregated_generated_class_array = np.hstack(aggregated_generated_class_list)\n",
    "    \n",
    "    train_x = aggregated_generated_image_array\n",
    "    train_y = aggregated_generated_class_array\n",
    "    \n",
    "    \n",
    "    # Creating testing data\n",
    "    print 'Generating testing data'\n",
    "    aggregated_generated_image_list = []\n",
    "    aggregated_generated_class_list = []\n",
    "    \n",
    "    for pair_index, pair in enumerate(image_class_pairs):\n",
    "        print 'Generating images for image', (pair_index + 1), ' out of', len(image_class_pairs)\n",
    "        \n",
    "        sampled_rotations = np.random.normal(mu, sigma, test_set_size) * 360\n",
    "        generated_image_list = []\n",
    "        generated_class_list = []\n",
    "        \n",
    "        for index, theta in enumerate(sampled_rotations):\n",
    "            normalizing_size = calculate_normalizing_dimensions(main_image_pair[0], theta)\n",
    "            sample = generate_sample(pair[0],\n",
    "                                     theta,\n",
    "                                     normalizing_size)\n",
    "            generated_image_list.append(sample)\n",
    "            generated_class_list.append(pair[1])\n",
    "            if 0 == (index + 1) % 10:\n",
    "                print 'Generated', (index + 1), 'samples out of', train_set_size, '.'\n",
    "    \n",
    "        combined_generated_image_array = np.vstack(generated_image_list)\n",
    "        aggregated_generated_image_list.append(combined_generated_image_array)\n",
    "        \n",
    "        combined_generated_class_array = np.array(generated_class_list)\n",
    "        aggregated_generated_class_list.append(combined_generated_class_array)\n",
    "        \n",
    "    aggregated_generated_image_array = np.vstack(aggregated_generated_image_list)\n",
    "    aggregated_generated_class_array = np.hstack(aggregated_generated_class_list)\n",
    "    \n",
    "    test_x = aggregated_generated_image_array\n",
    "    test_y = aggregated_generated_class_array\n",
    "    \n",
    "    return train_x, train_y, test_x, test_y\n",
    "\n",
    "tr_x, tr_y, te_x, te_y = generate_data_sets(image_class_list, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(160, 784)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from autograd import grad\n",
    "#import autograd.numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# DO NOT RUN THIS! HERE FOR EXAMPLE ONLY!\n",
    "\n",
    "import cPickle, gzip, numpy\n",
    "\n",
    "# Load the dataset\n",
    "f = gzip.open('mnist.pkl.gz', 'rb')\n",
    "train_set, valid_set, test_set = cPickle.load(f)\n",
    "f.close()\n",
    "\n",
    "def shared_dataset(data_xy):\n",
    "    \"\"\" Function that loads the dataset into shared variables\n",
    "\n",
    "    The reason we store our dataset in shared variables is to allow\n",
    "    Theano to copy it into the GPU memory (when code is run on GPU).\n",
    "    Since copying data into the GPU is slow, copying a minibatch everytime\n",
    "    is needed (the default behaviour if the data is not in a shared\n",
    "    variable) would lead to a large decrease in performance.\n",
    "    \"\"\"\n",
    "    data_x, data_y = data_xy\n",
    "    #shared_x = theano.shared(numpy.asarray(data_x, dtype=theano.config.floatX))\n",
    "    #shared_y = theano.shared(numpy.asarray(data_y, dtype=theano.config.floatX))\n",
    "    # When storing data on the GPU it has to be stored as floats\n",
    "    # therefore we will store the labels as ``floatX`` as well\n",
    "    # (``shared_y`` does exactly that). But during our computations\n",
    "    # we need them as ints (we use labels as index, and if they are\n",
    "    # floats it doesn't make sense) therefore instead of returning\n",
    "    # ``shared_y`` we will have to cast it to int. This little hack\n",
    "    # lets us get around this issue\n",
    "    #return shared_x, T.cast(shared_y, 'int32')\n",
    "    return data_x, data_y\n",
    "\n",
    "test_set_x, test_set_y = shared_dataset(test_set)\n",
    "valid_set_x, valid_set_y = shared_dataset(valid_set)\n",
    "train_set_x, train_set_y = shared_dataset(train_set)\n",
    "\n",
    "batch_size = 500    # size of the minibatch\n",
    "\n",
    "# accessing the third minibatch of the training set\n",
    "\n",
    "data  = train_set_x[2 * batch_size: 3 * batch_size]\n",
    "label = train_set_y[2 * batch_size: 3 * batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "\n",
    "def linear_activation(W, b, X):\n",
    "    \"\"\"\n",
    "    :param - W - m input nodes (one for each feature) with c output nodes (one for each class)\n",
    "    :param - b - c bias weight nodes (one for each class)\n",
    "    :param - X - n samples with m features each, nxm matrix\n",
    "    \"\"\"\n",
    "    return np.dot(X, W) + b\n",
    "\n",
    "# P(Y = i | x, W, b)\n",
    "# p_y_given_x\n",
    "def softmax(W, b, X):\n",
    "    \"\"\"\n",
    "    :param - W - m input nodes (one for each feature) with c output nodes (one for each class)\n",
    "    :param - b - c bias weight nodes (one for each class)\n",
    "    :param - X - n samples with m features each, nxm matrix\n",
    "    \"\"\"\n",
    "    return np.exp(np.dot(X, W) + b) / np.sum(np.exp(np.dot(X, W) + b), axis=1)[:, None]\n",
    "\n",
    "# y_pred\n",
    "def prediction(W, b, X):\n",
    "    \"\"\"\n",
    "    :param - W - m input nodes (one for each feature) with c output nodes (one for each class)\n",
    "    :param - b - c bias weight nodes (one for each class)\n",
    "    :param - X - n samples with m features each, nxm matrix\n",
    "    \"\"\"\n",
    "    return np.argmax(softmax(W, b, X), axis=1)\n",
    "\n",
    "def likelihood(W, b, X, Y):\n",
    "    \"\"\"\n",
    "    :param - W - m input nodes (one for each feature) with c output nodes (one for each class)\n",
    "    :param - b - c bias weight nodes (one for each class)\n",
    "    :param - X - n samples with m features each, nxm matrix\n",
    "    :param - Y - n classifications (one for each sample)\n",
    "    \"\"\"\n",
    "    return softmax(W, b, X)[np.arange(Y.shape[0]), Y]\n",
    "\n",
    "def negative_log_likelihood(W, b, X, Y):\n",
    "    \"\"\"\n",
    "    :param - W - m input nodes (one for each feature) with c output nodes (one for each class)\n",
    "    :param - b - c bias weight nodes (one for each class)\n",
    "    :param - X - n samples with m features each, nxm matrix\n",
    "    :param - Y - n classifications (one for each sample)\n",
    "    \"\"\"    \n",
    "    return -likelihood(W, b, X, Y)\n",
    "\n",
    "def loss(W, b, X, Y):\n",
    "    \"\"\"\n",
    "    :param - W - m input nodes (one for each feature) with c output nodes (one for each class)\n",
    "    :param - b - c bias weight nodes (one for each class)\n",
    "    :param - X - n samples with m features each, nxm matrix\n",
    "    :param - Y - n classifications (one for each sample)\n",
    "    \"\"\"\n",
    "    return np.mean(negative_log_likelihood(W, b, X, Y))\n",
    "\n",
    "def errors(W, b, X, Y):\n",
    "    \"\"\"\n",
    "    :param - W - m input nodes (one for each feature) with c output nodes (one for each class)\n",
    "    :param - b - c bias weight nodes (one for each class)\n",
    "    :param - X - n samples with m features each, nxm matrix\n",
    "    :param - Y - n classifications (one for each sample)\n",
    "    \"\"\"\n",
    "    return np.mean(np.not_equal(prediction(W, b, X), Y))\n",
    "\n",
    "loss_grad_W = grad(loss, 0)\n",
    "loss_grad_b = grad(loss, 1)\n",
    "\n",
    "class LogisticRegression():\n",
    "    def __init__(self, n_in, n_out, learning_rate=0.13):\n",
    "        self.W = np.random.rand(n_in * n_out).reshape(n_in, n_out)\n",
    "        self.b = np.zeros((n_out,), dtype=np.float64)\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "def logistic_regression_update_values(lr, X, Y):\n",
    "    W_update = lr.W - lr.learning_rate * loss_grad_W(lr.W, lr.b, X, Y)\n",
    "    b_update = lr.b - lr.learning_rate * loss_grad_b(lr.W, lr.b, X, Y)\n",
    "    \n",
    "    return W_update, b_update\n",
    "        \n",
    "    \n",
    "def logistic_regression_training_environment(lr, data, labels, epochs=100, batch_size=500):\n",
    "    n_train_batches = data.shape[0] // batch_size\n",
    "    \n",
    "    for epoch_count in range(epochs):\n",
    "        for index in range(n_train_batches):\n",
    "            X_batch = data[index * batch_size: (index + 1) * batch_size]\n",
    "            Y_batch = labels[index * batch_size: (index + 1) * batch_size]\n",
    "            \n",
    "            lr.W, lr.b = logistic_regression_update_values(lr, X_batch, Y_batch)\n",
    "            \n",
    "        if 0 == epoch_count % 10:\n",
    "            print 'Epoch:', epoch_count, ', Error rate:', errors(lr.W, lr.b, data, labels)\n",
    "            \n",
    "def logistic_regression_demo():\n",
    "    from autograd import grad\n",
    "    import autograd.numpy as np\n",
    "    import cPickle, gzip, numpy\n",
    "\n",
    "    # Load the dataset\n",
    "    demo_f = gzip.open('mnist.pkl.gz', 'rb')\n",
    "    demo_train_set, demo_valid_set, demo_test_set = cPickle.load(demo_f)\n",
    "    demo_f.close()\n",
    "\n",
    "    demo_test_set_x, demo_test_set_y = demo_test_set\n",
    "    demo_valid_set_x, demo_valid_set_y = demo_valid_set\n",
    "    demo_train_set_x, demo_train_set_y = demo_train_set\n",
    "\n",
    "    demo_n_in = 28*28\n",
    "    demo_n_out = 10\n",
    "    demo_learning_rate = 0.13\n",
    "    \n",
    "    demo_c = LogisticRegression(demo_n_in, demo_n_out, demo_learning_rate)\n",
    "    \n",
    "    print \"Baseline:\", errors(c.W, c.b, demo_test_set_x, demo_test_set_y)\n",
    "    \n",
    "    logistic_regression_training_environment(demo_c, demo_train_set_x, demo_train_set_y, 1000)\n",
    "    \n",
    "    print \"End of Training:\", errors(c.W, c.b, demo_test_set_x, demo_test_set_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chr_in = 28*28\n",
    "chr_out = len(image_class_list)\n",
    "chr_learning_rate = 0.13\n",
    "\n",
    "c = LogisticRegression(chr_in, chr_out, chr_learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline: 0.9375\n"
     ]
    }
   ],
   "source": [
    "print 'Baseline:', errors(c.W, c.b, te_x, te_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 , Error rate: 0.90625\n",
      "Epoch: 10 , Error rate: 0.83125\n",
      "Epoch: 20 , Error rate: 0.775\n",
      "Epoch: 30 , Error rate: 0.70625\n",
      "Epoch: 40 , Error rate: 0.65625\n",
      "Epoch: 50 , Error rate: 0.6125\n",
      "Epoch: 60 , Error rate: 0.58125\n",
      "Epoch: 70 , Error rate: 0.575\n",
      "Epoch: 80 , Error rate: 0.575\n",
      "Epoch: 90 , Error rate: 0.55625\n",
      "Epoch: 100 , Error rate: 0.5375\n",
      "Epoch: 110 , Error rate: 0.53125\n",
      "Epoch: 120 , Error rate: 0.53125\n",
      "Epoch: 130 , Error rate: 0.53125\n",
      "Epoch: 140 , Error rate: 0.525\n",
      "Epoch: 150 , Error rate: 0.50625\n",
      "Epoch: 160 , Error rate: 0.5\n",
      "Epoch: 170 , Error rate: 0.49375\n",
      "Epoch: 180 , Error rate: 0.49375\n",
      "Epoch: 190 , Error rate: 0.49375\n",
      "Epoch: 200 , Error rate: 0.49375\n",
      "Epoch: 210 , Error rate: 0.49375\n",
      "Epoch: 220 , Error rate: 0.49375\n",
      "Epoch: 230 , Error rate: 0.4875\n",
      "Epoch: 240 , Error rate: 0.48125\n",
      "Epoch: 250 , Error rate: 0.48125\n",
      "Epoch: 260 , Error rate: 0.46875\n",
      "Epoch: 270 , Error rate: 0.4625\n",
      "Epoch: 280 , Error rate: 0.4625\n",
      "Epoch: 290 , Error rate: 0.45625\n",
      "Epoch: 300 , Error rate: 0.45625\n",
      "Epoch: 310 , Error rate: 0.45625\n",
      "Epoch: 320 , Error rate: 0.45625\n",
      "Epoch: 330 , Error rate: 0.45625\n",
      "Epoch: 340 , Error rate: 0.45625\n",
      "Epoch: 350 , Error rate: 0.45625\n",
      "Epoch: 360 , Error rate: 0.45625\n",
      "Epoch: 370 , Error rate: 0.45625\n",
      "Epoch: 380 , Error rate: 0.45625\n",
      "Epoch: 390 , Error rate: 0.45625\n",
      "Epoch: 400 , Error rate: 0.45\n",
      "Epoch: 410 , Error rate: 0.45\n",
      "Epoch: 420 , Error rate: 0.45\n",
      "Epoch: 430 , Error rate: 0.44375\n",
      "Epoch: 440 , Error rate: 0.44375\n",
      "Epoch: 450 , Error rate: 0.44375\n",
      "Epoch: 460 , Error rate: 0.44375\n",
      "Epoch: 470 , Error rate: 0.44375\n",
      "Epoch: 480 , Error rate: 0.44375\n",
      "Epoch: 490 , Error rate: 0.44375\n",
      "Epoch: 500 , Error rate: 0.44375\n",
      "Epoch: 510 , Error rate: 0.44375\n",
      "Epoch: 520 , Error rate: 0.4375\n",
      "Epoch: 530 , Error rate: 0.4375\n",
      "Epoch: 540 , Error rate: 0.4375\n",
      "Epoch: 550 , Error rate: 0.4375\n",
      "Epoch: 560 , Error rate: 0.4375\n",
      "Epoch: 570 , Error rate: 0.4375\n",
      "Epoch: 580 , Error rate: 0.4375\n",
      "Epoch: 590 , Error rate: 0.4375\n",
      "Epoch: 600 , Error rate: 0.4375\n",
      "Epoch: 610 , Error rate: 0.425\n",
      "Epoch: 620 , Error rate: 0.425\n",
      "Epoch: 630 , Error rate: 0.425\n",
      "Epoch: 640 , Error rate: 0.425\n",
      "Epoch: 650 , Error rate: 0.41875\n",
      "Epoch: 660 , Error rate: 0.41875\n",
      "Epoch: 670 , Error rate: 0.41875\n",
      "Epoch: 680 , Error rate: 0.41875\n",
      "Epoch: 690 , Error rate: 0.41875\n",
      "Epoch: 700 , Error rate: 0.41875\n",
      "Epoch: 710 , Error rate: 0.41875\n",
      "Epoch: 720 , Error rate: 0.41875\n",
      "Epoch: 730 , Error rate: 0.41875\n",
      "Epoch: 740 , Error rate: 0.41875\n",
      "Epoch: 750 , Error rate: 0.41875\n",
      "Epoch: 760 , Error rate: 0.41875\n",
      "Epoch: 770 , Error rate: 0.41875\n",
      "Epoch: 780 , Error rate: 0.41875\n",
      "Epoch: 790 , Error rate: 0.41875\n",
      "Epoch: 800 , Error rate: 0.41875\n",
      "Epoch: 810 , Error rate: 0.41875\n",
      "Epoch: 820 , Error rate: 0.41875\n",
      "Epoch: 830 , Error rate: 0.41875\n",
      "Epoch: 840 , Error rate: 0.41875\n",
      "Epoch: 850 , Error rate: 0.41875\n",
      "Epoch: 860 , Error rate: 0.41875\n",
      "Epoch: 870 , Error rate: 0.41875\n",
      "Epoch: 880 , Error rate: 0.41875\n",
      "Epoch: 890 , Error rate: 0.41875\n",
      "Epoch: 900 , Error rate: 0.41875\n",
      "Epoch: 910 , Error rate: 0.41875\n",
      "Epoch: 920 , Error rate: 0.41875\n",
      "Epoch: 930 , Error rate: 0.41875\n",
      "Epoch: 940 , Error rate: 0.41875\n",
      "Epoch: 950 , Error rate: 0.41875\n",
      "Epoch: 960 , Error rate: 0.41875\n",
      "Epoch: 970 , Error rate: 0.41875\n",
      "Epoch: 980 , Error rate: 0.41875\n",
      "Epoch: 990 , Error rate: 0.41875\n"
     ]
    }
   ],
   "source": [
    "logistic_regression_training_environment(c, tr_x, tr_y, epochs=1000, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
